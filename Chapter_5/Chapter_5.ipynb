{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhankui/anaconda3/envs/pml/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.4.8'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import wget\n",
    "import gzip\n",
    "import random\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "import torch \n",
    "from torch import nn\n",
    "from collections import defaultdict\n",
    "from implicit import bpr\n",
    "from surprise import SVD, Reader, Dataset\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "import implicit\n",
    "implicit.__version__ # implicit 0.5.2 will report errors, we use 0.4.8 instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is available at http://cseweb.ucsd.edu/~jmcauley/pml/data/. \n",
    "- Download and save to your own directory.\n",
    "- Or, run following script to save it into `Chapter_5/data` folder automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "filenames = [\n",
    "    'goodreads_fantasy.tsv', \n",
    "    'goodreads_reviews_fantasy_paranormal.json.gz', \n",
    "    'goodreads_reviews_comics_graphic.json.gz'\n",
    "]\n",
    "\n",
    "dataDir = './data'\n",
    "url = 'http://jmcauley.ucsd.edu/pml_data'\n",
    "\n",
    "if not os.path.exists(dataDir):\n",
    "    os.makedirs(dataDir)\n",
    "for filename in filenames:\n",
    "    wget.download(os.path.join(url, filename), out=dataDir)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent factor model (`Surprise`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the library's inbuilt data reader, extract tsv-formatted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(line_format='user item rating', sep='\\t')\n",
    "data = Dataset.load_from_file(os.path.join(dataDir, \"goodreads_fantasy.tsv\"), reader=reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard latent-factor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inbuilt functions to split into training and test fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size=.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model and extract predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(trainset)\n",
    "predictions = model.test(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate for a single (test) rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.41674281295034"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE for model predictions (test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.18535715288669\n"
     ]
    }
   ],
   "source": [
    "sse = 0\n",
    "for p in predictions:\n",
    "    sse += (p.r_ui - p.est)**2\n",
    "\n",
    "print(sse / len(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Personalized Ranking (`Implicit`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseData(fname):\n",
    "    for l in gzip.open(fname):\n",
    "        d = eval(l)\n",
    "        del d['review_text'] # Discard the reviews, to save memory when we don't use them\n",
    "        yield d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full dataset of Goodreads fantasy reviews (fairly memory-hungry, could be replaced by something smaller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(parseData(os.path.join(dataDir, \"goodreads_reviews_fantasy_paranormal.json.gz\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': '660fecee4a96d682c821d0f36b56d073',\n",
       " 'book_id': '13648459',\n",
       " 'review_id': '7f59659b0f3ea6fdcf0943699bfd361d',\n",
       " 'rating': 5,\n",
       " 'date_added': 'Sun Apr 14 19:20:01 -0700 2013',\n",
       " 'date_updated': 'Sun Apr 14 19:21:12 -0700 2013',\n",
       " 'read_at': 'Sun Apr 14 00:00:00 -0700 2013',\n",
       " 'started_at': '',\n",
       " 'n_votes': 2,\n",
       " 'n_comments': 0}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a few utility data structures. Since we'll be converting the data to a sparse interaction matrix, the main structure here is to assign each user/item to an ID from 0 to nUsers/nItems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "userIDs,itemIDs = {},{}\n",
    "\n",
    "for d in data:\n",
    "    u,i = d['user_id'],d['book_id']\n",
    "    if not u in userIDs: userIDs[u] = len(userIDs)\n",
    "    if not i in itemIDs: itemIDs[i] = len(itemIDs)\n",
    "\n",
    "nUsers,nItems = len(userIDs),len(itemIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256088, 258212)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nUsers,nItems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert dataset to sparse matrix. Only storing positive feedback instances (i.e., rated items)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xiu = scipy.sparse.lil_matrix((nItems, nUsers))\n",
    "for d in data:\n",
    "    Xiu[itemIDs[d['book_id']],userIDs[d['user_id']]] = 1\n",
    "    \n",
    "Xui = Xiu.T.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Personalized Ranking model with 5 latent factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bpr.BayesianPersonalizedRanking(factors = 5, use_gpu = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:11<00:00,  8.89it/s, train_auc=90.07%, skipped=1.81%]\n"
     ]
    }
   ],
   "source": [
    "model.fit(Xiu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get recommendations for a particular user (the first one) and to get items related to (similar latent factors) to a particular item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended = model.recommend(0, Xui)\n",
    "related = model.similar_items(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1.0000001),\n",
       " (46541, 0.9904368),\n",
       " (112012, 0.98807365),\n",
       " (114601, 0.98681265),\n",
       " (63634, 0.9843552),\n",
       " (154377, 0.9819736),\n",
       " (55813, 0.981672),\n",
       " (136790, 0.98098093),\n",
       " (7517, 0.9808126),\n",
       " (99516, 0.9805903)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract user and item factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemFactors = model.item_factors\n",
    "userFactors = model.user_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.1206227 , -0.6256624 , -0.8490893 ,  0.689599  , -0.50638354,\n",
       "       -0.19383602], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemFactors[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent factor model (`TensorFlow` and `PyTorch`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goodreads comic book data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "userIDs = {}\n",
    "itemIDs = {}\n",
    "interactions = []\n",
    "\n",
    "for d in parse(os.path.join(dataDir, \"goodreads_reviews_comics_graphic.json.gz\")):\n",
    "    u = d['user_id']\n",
    "    i = d['book_id']\n",
    "    r = d['rating']\n",
    "    if not u in userIDs: userIDs[u] = len(userIDs)\n",
    "    if not i in itemIDs: itemIDs[i] = len(itemIDs)\n",
    "    interactions.append((u,i,r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "542338"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(interactions)\n",
    "len(interactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTrain = int(len(interactions) * 0.9)\n",
    "nTest = len(interactions) - nTrain\n",
    "interactionsTrain = interactions[:nTrain]\n",
    "interactionsTest = interactions[nTrain:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsPerUser = defaultdict(list)\n",
    "usersPerItem = defaultdict(list)\n",
    "for u,i,r in interactionsTrain:\n",
    "    itemsPerUser[u].append(i)\n",
    "    usersPerItem[i].append(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean rating, just for initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = sum([r for _,_,r in interactionsTrain]) / len(interactionsTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `TensorFlow` Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent optimizer, could experiment with learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent factor model tensorflow class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentFactorModel(tf.keras.Model):\n",
    "    def __init__(self, mu, K, lamb):\n",
    "        super(LatentFactorModel, self).__init__()\n",
    "        # Initialize to average\n",
    "        self.alpha = tf.Variable(mu)\n",
    "        # Initialize to small random values\n",
    "        self.betaU = tf.Variable(tf.random.normal([len(userIDs)],stddev=0.001))\n",
    "        self.betaI = tf.Variable(tf.random.normal([len(itemIDs)],stddev=0.001))\n",
    "        self.gammaU = tf.Variable(tf.random.normal([len(userIDs),K],stddev=0.001))\n",
    "        self.gammaI = tf.Variable(tf.random.normal([len(itemIDs),K],stddev=0.001))\n",
    "        self.lamb = lamb\n",
    "\n",
    "    # Prediction for a single instance (useful for evaluation)\n",
    "    def predict(self, u, i):\n",
    "        p = self.alpha + self.betaU[u] + self.betaI[i] +\\\n",
    "            tf.tensordot(self.gammaU[u], self.gammaI[i], 1)\n",
    "        return p\n",
    "\n",
    "    # Regularizer\n",
    "    def reg(self):\n",
    "        return self.lamb * (tf.reduce_sum(self.betaU**2) +\\\n",
    "                            tf.reduce_sum(self.betaI**2) +\\\n",
    "                            tf.reduce_sum(self.gammaU**2) +\\\n",
    "                            tf.reduce_sum(self.gammaI**2))\n",
    "    \n",
    "    # Prediction for a sample of instances\n",
    "    def predictSample(self, sampleU, sampleI):\n",
    "        u = tf.convert_to_tensor(sampleU, dtype=tf.int32)\n",
    "        i = tf.convert_to_tensor(sampleI, dtype=tf.int32)\n",
    "        beta_u = tf.nn.embedding_lookup(self.betaU, u)\n",
    "        beta_i = tf.nn.embedding_lookup(self.betaI, i)\n",
    "        gamma_u = tf.nn.embedding_lookup(self.gammaU, u)\n",
    "        gamma_i = tf.nn.embedding_lookup(self.gammaI, i)\n",
    "        pred = self.alpha + beta_u + beta_i +\\\n",
    "               tf.reduce_sum(tf.multiply(gamma_u, gamma_i), 1)\n",
    "        return pred\n",
    "    \n",
    "    # Loss\n",
    "    def call(self, sampleU, sampleI, sampleR):\n",
    "        pred = self.predictSample(sampleU, sampleI)\n",
    "        r = tf.convert_to_tensor(sampleR, dtype=tf.float32)\n",
    "        return tf.nn.l2_loss(pred - r) / len(sampleR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model. Could experiment with number of factors and regularization rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLFM = LatentFactorModel(mu, 5, 0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training step (for the batch-based model from Chapter 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingStep(model, interactions):\n",
    "    Nsamples = 50000\n",
    "    with tf.GradientTape() as tape:\n",
    "        sampleU, sampleI, sampleR = [], [], []\n",
    "        for _ in range(Nsamples):\n",
    "            u,i,r = random.choice(interactions)\n",
    "            sampleU.append(userIDs[u])\n",
    "            sampleI.append(itemIDs[i])\n",
    "            sampleR.append(r)\n",
    "\n",
    "        loss = model(sampleU,sampleI,sampleR)\n",
    "        loss += model.reg()\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients((grad, var) for\n",
    "                              (grad, var) in zip(gradients, model.trainable_variables)\n",
    "                              if grad is not None)\n",
    "    return loss.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 100 iterations (really 100 batches) of gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10, objective = 0.5413822\n",
      "iteration 20, objective = 0.513759\n",
      "iteration 30, objective = 0.5110235\n",
      "iteration 40, objective = 0.51673234\n",
      "iteration 50, objective = 0.5165422\n",
      "iteration 60, objective = 0.5135466\n",
      "iteration 70, objective = 0.51165915\n",
      "iteration 80, objective = 0.51350456\n",
      "iteration 90, objective = 0.51042646\n",
      "iteration 100, objective = 0.5097919\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    obj = trainingStep(modelLFM, interactionsTrain)\n",
    "    if (i % 10 == 9): print(\"iteration \" + str(i+1) + \", objective = \" + str(obj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction for a particular user/item pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "u,i,r = interactionsTest[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLFM.predict(userIDs[u], itemIDs[i]).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `PyTorch` Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent factor model tensorflow class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentFactorModel(nn.Module):\n",
    "    def __init__(self, mu, K, lamb):\n",
    "        super(LatentFactorModel, self).__init__()\n",
    "        # Initialize to average\n",
    "        self.alpha = nn.Parameter(torch.Tensor([mu]))\n",
    "        # Initialize to small random values\n",
    "        self.betaU = nn.Embedding(len(userIDs), 1)\n",
    "        nn.init.normal_(self.betaU.weight, std=0.001)\n",
    "        self.betaI = nn.Embedding(len(itemIDs), 1)\n",
    "        nn.init.normal_(self.betaI.weight, std=0.001)\n",
    "        self.gammaU = nn.Embedding(len(userIDs), K)\n",
    "        nn.init.normal_(self.gammaU.weight, std=0.001)\n",
    "        self.gammaI = nn.Embedding(len(itemIDs), K)\n",
    "        nn.init.normal_(self.gammaI.weight, std=0.001)\n",
    "        self.lamb = lamb\n",
    "\n",
    "    # Prediction\n",
    "    def forward(self, u, i):\n",
    "        p = self.alpha + self.betaU(u).squeeze(-1) + self.betaI(i).squeeze(-1) + \\\n",
    "            (self.gammaU(u) * self.gammaI(i)).sum(dim=-1)\n",
    "        return p\n",
    "\n",
    "    # Regularizer\n",
    "    def reg(self):\n",
    "        return self.lamb * (torch.sum(self.betaU.weight**2) + \\\n",
    "                            torch.sum(self.betaI.weight**2) + \\\n",
    "                            torch.sum(self.gammaU.weight**2) + \\\n",
    "                            torch.sum(self.gammaI.weight**2))\n",
    "    \n",
    "    # Loss\n",
    "    def loss(self, u, i, r):\n",
    "        pred = self.forward(u, i)\n",
    "        return nn.functional.mse_loss(pred, r) + self.reg() # mse_loss in pytorch is 2 * l2_loss in tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model. Could experiment with number of factors and regularization rate.\n",
    "\n",
    "Gradient descent optimizer, could experiment with learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLFM = LatentFactorModel(mu, 5, 0.00001)\n",
    "optimizer = torch.optim.Adam(modelLFM.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training step (for the batch-based model from Chapter 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trainingStep(model, interactions):  \n",
    "    # gradient reset\n",
    "    optimizer.zero_grad() \n",
    "    # data generation\n",
    "    Nsamples = 50000\n",
    "    sampleU, sampleI, sampleR = [], [], []\n",
    "    for _ in range(Nsamples):\n",
    "        u,i,r = random.choice(interactions)\n",
    "        sampleU.append(userIDs[u])\n",
    "        sampleI.append(itemIDs[i])\n",
    "        sampleR.append(r)\n",
    "    sampleU, sampleI, sampleR = torch.LongTensor(sampleU), torch.LongTensor(sampleI), torch.Tensor(sampleR)\n",
    "    # loss calculation\n",
    "    loss = model.loss(sampleU, sampleI, sampleR)\n",
    "    # gradient calculation\n",
    "    loss.backward()\n",
    "    # weight update\n",
    "    optimizer.step()\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 100 iterations (really 100 batches) of gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(100):\n",
    "    loss = trainingStep(modelLFM, interactionsTrain)\n",
    "    if (i % 10 == 9): print(\"iteration \" + str(i+1) + \", loss = \" + str(loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction for a particular user/item pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "u,i,r = interactionsTest[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.046658039093018"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelLFM(torch.LongTensor([userIDs[u]]), torch.LongTensor([itemIDs[i]])).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian personalized ranking (`TensorFlow` and `PyTorch`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = list(itemIDs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `TensorFlow` Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch-based version from Chapter 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPRbatch(tf.keras.Model):\n",
    "    def __init__(self, K, lamb):\n",
    "        super(BPRbatch, self).__init__()\n",
    "        # Initialize variables\n",
    "        self.betaI = tf.Variable(tf.random.normal([len(itemIDs)],stddev=0.001))\n",
    "        self.gammaU = tf.Variable(tf.random.normal([len(userIDs),K],stddev=0.001))\n",
    "        self.gammaI = tf.Variable(tf.random.normal([len(itemIDs),K],stddev=0.001))\n",
    "        # Regularization coefficient\n",
    "        self.lamb = lamb\n",
    "\n",
    "    # Prediction for a single instance\n",
    "    def predict(self, u, i):\n",
    "        p = self.betaI[i] + tf.tensordot(self.gammaU[u], self.gammaI[i], 1)\n",
    "        return p\n",
    "\n",
    "    # Regularizer\n",
    "    def reg(self):\n",
    "        return self.lamb * (tf.nn.l2_loss(self.betaI) +\\\n",
    "                            tf.nn.l2_loss(self.gammaU) +\\\n",
    "                            tf.nn.l2_loss(self.gammaI))\n",
    "    \n",
    "    def score(self, sampleU, sampleI):\n",
    "        u = tf.convert_to_tensor(sampleU, dtype=tf.int32)\n",
    "        i = tf.convert_to_tensor(sampleI, dtype=tf.int32)\n",
    "        beta_i = tf.nn.embedding_lookup(self.betaI, i)\n",
    "        gamma_u = tf.nn.embedding_lookup(self.gammaU, u)\n",
    "        gamma_i = tf.nn.embedding_lookup(self.gammaI, i)\n",
    "        x_ui = beta_i + tf.reduce_sum(tf.multiply(gamma_u, gamma_i), 1)\n",
    "        return x_ui\n",
    "\n",
    "    def call(self, sampleU, sampleI, sampleJ):\n",
    "        x_ui = self.score(sampleU, sampleI)\n",
    "        x_uj = self.score(sampleU, sampleJ)\n",
    "        return -tf.reduce_mean(tf.math.log(tf.math.sigmoid(x_ui - x_uj)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelBPR = BPRbatch(5, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingStepBPR(model, interactions):\n",
    "    Nsamples = 50000\n",
    "    with tf.GradientTape() as tape:\n",
    "        sampleU, sampleI, sampleJ = [], [], []\n",
    "        for _ in range(Nsamples):\n",
    "            u,i,_ = random.choice(interactions) # positive sample\n",
    "            j = random.choice(items) # negative sample\n",
    "            while j in itemsPerUser[u]:\n",
    "                j = random.choice(items)\n",
    "            sampleU.append(userIDs[u])\n",
    "            sampleI.append(itemIDs[i])\n",
    "            sampleJ.append(itemIDs[j])\n",
    "\n",
    "        loss = model(sampleU,sampleI,sampleJ)\n",
    "        loss += model.reg()\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients((grad, var) for\n",
    "                              (grad, var) in zip(gradients, model.trainable_variables)\n",
    "                              if grad is not None)\n",
    "    return loss.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 100 batches of gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10, objective = 0.5303632\n",
      "iteration 20, objective = 0.47583044\n",
      "iteration 30, objective = 0.4722096\n",
      "iteration 40, objective = 0.47280103\n",
      "iteration 50, objective = 0.47669053\n",
      "iteration 60, objective = 0.47630626\n",
      "iteration 70, objective = 0.47247666\n",
      "iteration 80, objective = 0.4745618\n",
      "iteration 90, objective = 0.47154313\n",
      "iteration 100, objective = 0.46945256\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    obj = trainingStepBPR(modelBPR, interactions)\n",
    "    if (i % 10 == 9): print(\"iteration \" + str(i+1) + \", objective = \" + str(obj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction for a particular user/item pair. Note that this is an unnormalized score (which can be used for ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "u,i,_ = interactionsTest[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6619426"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In this case just a score (that can be used for ranking), rather than a prediction of a rating\n",
    "modelBPR.predict(userIDs[u], itemIDs[i]).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `PyTorch` Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch-based version from Chapter 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPRbatch(nn.Module):\n",
    "    def __init__(self, K, lamb):\n",
    "        super(BPRbatch, self).__init__()\n",
    "        # Initialize to small random values\n",
    "        self.betaI = nn.Embedding(len(itemIDs), 1)\n",
    "        nn.init.normal_(self.betaI.weight, std=0.001)\n",
    "        self.gammaU = nn.Embedding(len(userIDs), K)\n",
    "        nn.init.normal_(self.gammaU.weight, std=0.001)\n",
    "        self.gammaI = nn.Embedding(len(itemIDs), K)\n",
    "        nn.init.normal_(self.gammaI.weight, std=0.001)\n",
    "        self.lamb = lamb\n",
    "\n",
    "    # Prediction\n",
    "    def forward(self, u, i):\n",
    "        p = self.betaI(i).squeeze(-1) + (self.gammaU(u) * self.gammaI(i)).sum(dim=-1)\n",
    "        return p\n",
    "\n",
    "    # Regularizer\n",
    "    def reg(self):\n",
    "        return self.lamb * (torch.sum(self.betaI.weight**2) + \\\n",
    "                            torch.sum(self.gammaU.weight**2) + \\\n",
    "                            torch.sum(self.gammaI.weight**2))\n",
    "    \n",
    "    # Loss\n",
    "    def loss(self, u, i, j):\n",
    "        x_ui = self.forward(u, i)\n",
    "        x_uj = self.forward(u, j)\n",
    "        return - torch.sigmoid(x_ui - x_uj).log().mean() + self.reg() # mse_loss in pytorch is 2 * l2_loss in tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelBPR = BPRbatch(5, 0.00001)\n",
    "optimizer = torch.optim.Adam(modelBPR.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingStepBPR(model, interactions):  \n",
    "    # gradient reset\n",
    "    optimizer.zero_grad() \n",
    "    # data generation\n",
    "    Nsamples = 50000\n",
    "    sampleU, sampleI, sampleJ = [], [], []\n",
    "    for _ in range(Nsamples):\n",
    "        u,i,_ = random.choice(interactions) # positive sample\n",
    "        j = random.choice(items) # negative sample\n",
    "        while j in itemsPerUser[u]:\n",
    "            j = random.choice(items)\n",
    "        sampleU.append(userIDs[u])\n",
    "        sampleI.append(itemIDs[i])\n",
    "        sampleJ.append(itemIDs[j])\n",
    "    sampleU, sampleI, sampleJ = torch.LongTensor(sampleU), torch.LongTensor(sampleI), torch.LongTensor(sampleJ)\n",
    "    # loss calculation\n",
    "    loss = model.loss(sampleU, sampleI, sampleJ)\n",
    "    # gradient calculation\n",
    "    loss.backward()\n",
    "    # weight update\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 100 batches of gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10, loss = 0.5781843066215515\n",
      "iteration 20, loss = 0.5346540212631226\n",
      "iteration 30, loss = 0.5334715843200684\n",
      "iteration 40, loss = 0.531741738319397\n",
      "iteration 50, loss = 0.5284981727600098\n",
      "iteration 60, loss = 0.529007077217102\n",
      "iteration 70, loss = 0.5291609764099121\n",
      "iteration 80, loss = 0.5280734896659851\n",
      "iteration 90, loss = 0.5251089930534363\n",
      "iteration 100, loss = 0.5288001298904419\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    loss = trainingStepBPR(modelBPR, interactionsTrain)\n",
    "    if (i % 10 == 9): print(\"iteration \" + str(i+1) + \", loss = \" + str(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction for a particular user/item pair. Note that this is an unnormalized score (which can be used for ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, i, _ = interactionsTest[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.779024362564087"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelLFM(torch.LongTensor([userIDs[u]]), torch.LongTensor([itemIDs[i]])).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 (`TensorFlow` Implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt the latent factor model above, simply deleting any terms associated with latent factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentFactorModelBiasOnly(tf.keras.Model):\n",
    "    def __init__(self, mu, lamb):\n",
    "        super(LatentFactorModelBiasOnly, self).__init__()\n",
    "        # Initialize to average\n",
    "        self.alpha = tf.Variable(mu)\n",
    "        # Initialize to small random values\n",
    "        self.betaU = tf.Variable(tf.random.normal([len(userIDs)],stddev=0.001))\n",
    "        self.betaI = tf.Variable(tf.random.normal([len(itemIDs)],stddev=0.001))\n",
    "        self.lamb = lamb\n",
    "\n",
    "    # Prediction for a single instance (useful for evaluation)\n",
    "    def predict(self, u, i):\n",
    "        p = self.alpha + self.betaU[u] + self.betaI[i]\n",
    "        return p\n",
    "\n",
    "    # Regularizer\n",
    "    def reg(self):\n",
    "        return self.lamb * (tf.reduce_sum(self.betaU**2) +\\\n",
    "                            tf.reduce_sum(self.betaI**2))\n",
    "    \n",
    "    # Prediction for a sample of instances\n",
    "    def predictSample(self, sampleU, sampleI):\n",
    "        u = tf.convert_to_tensor(sampleU, dtype=tf.int32)\n",
    "        i = tf.convert_to_tensor(sampleI, dtype=tf.int32)\n",
    "        beta_u = tf.nn.embedding_lookup(self.betaU, u)\n",
    "        beta_i = tf.nn.embedding_lookup(self.betaI, i)\n",
    "        pred = self.alpha + beta_u + beta_i\n",
    "        return pred\n",
    "    \n",
    "    # Loss\n",
    "    def call(self, sampleU, sampleI, sampleR):\n",
    "        pred = self.predictSample(sampleU, sampleI)\n",
    "        r = tf.convert_to_tensor(sampleR, dtype=tf.float32)\n",
    "        return tf.nn.l2_loss(pred - r) / len(sampleR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelBiasOnly = LatentFactorModelBiasOnly(mu, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingStepBiasOnly(model, interactions):\n",
    "    Nsamples = 50000\n",
    "    with tf.GradientTape() as tape:\n",
    "        sampleU, sampleI, sampleR = [], [], []\n",
    "        for _ in range(Nsamples):\n",
    "            u,i,r = random.choice(interactions)\n",
    "            sampleU.append(userIDs[u])\n",
    "            sampleI.append(itemIDs[i])\n",
    "            sampleR.append(r)\n",
    "\n",
    "        loss = model(sampleU,sampleI,sampleR)\n",
    "        loss += model.reg()\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients((grad, var) for\n",
    "        (grad, var) in zip(gradients, model.trainable_variables)\n",
    "        if grad is not None)\n",
    "    return loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10, objective = 0.57053554\n",
      "iteration 20, objective = 0.5389685\n",
      "iteration 30, objective = 0.5308695\n",
      "iteration 40, objective = 0.52404946\n",
      "iteration 50, objective = 0.516751\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    obj = trainingStepBiasOnly(modelBiasOnly, interactionsTrain)\n",
    "    if (i % 10 == 9): print(\"iteration \" + str(i+1) + \", objective = \" + str(obj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the MSEs for a model which always predicts the mean, versus one which involves bias terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "alwaysPredictMean = [mu for _ in interactionsTest]\n",
    "labels = [r for _,_,r in interactionsTest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3267613992499294"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(alwaysPredictMean, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "biasOnlyPredictions =\\\n",
    "    [modelBiasOnly.predict(userIDs[u],itemIDs[i]).numpy() for u,i,_ in interactionsTest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.478474"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biasOnlyPredictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0027213071269891"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(biasOnlyPredictions, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 (`PyTorch` Implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10, loss = 1.0379294157028198\n",
      "iteration 20, loss = 0.9627811908721924\n",
      "iteration 30, loss = 0.9267162084579468\n",
      "iteration 40, loss = 0.925595760345459\n",
      "iteration 50, loss = 0.9355825781822205\n"
     ]
    }
   ],
   "source": [
    "class LatentFactorModelBiasOnly(nn.Module):\n",
    "    def __init__(self, mu, K, lamb):\n",
    "        super(LatentFactorModelBiasOnly, self).__init__()\n",
    "        # Initialize to average\n",
    "        self.alpha = nn.Parameter(torch.Tensor([mu]))\n",
    "        # Initialize to small random values\n",
    "        self.betaU = nn.Embedding(len(userIDs), 1)\n",
    "        nn.init.normal_(self.betaU.weight, std=0.001)\n",
    "        self.betaI = nn.Embedding(len(itemIDs), 1)\n",
    "        nn.init.normal_(self.betaI.weight, std=0.001)\n",
    "        self.lamb = lamb\n",
    "\n",
    "    # Prediction\n",
    "    def forward(self, u, i):\n",
    "        p = self.alpha + self.betaU(u).squeeze(-1) + self.betaI(i).squeeze(-1)\n",
    "        return p\n",
    "\n",
    "    # Regularizer\n",
    "    def reg(self):\n",
    "        return self.lamb * (torch.sum(self.betaU.weight**2) + \\\n",
    "                            torch.sum(self.betaI.weight**2))\n",
    "    \n",
    "    # Loss\n",
    "    def loss(self, u, i, r):\n",
    "        pred = self.forward(u, i)\n",
    "        return nn.functional.mse_loss(pred, r) + self.reg() # mse_loss in pytorch is 2 * l2_loss in tensorflow\n",
    "\n",
    "modelBiasOnly = LatentFactorModelBiasOnly(mu, 5, 0.00001)\n",
    "optimizer = torch.optim.Adam(modelBiasOnly.parameters(), lr=0.1)\n",
    "\n",
    "def trainingStepBiasOnly(model, interactions):  \n",
    "    # gradient reset\n",
    "    optimizer.zero_grad() \n",
    "    # data generation\n",
    "    Nsamples = 50000\n",
    "    sampleU, sampleI, sampleR = [], [], []\n",
    "    for _ in range(Nsamples):\n",
    "        u,i,r = random.choice(interactions)\n",
    "        sampleU.append(userIDs[u])\n",
    "        sampleI.append(itemIDs[i])\n",
    "        sampleR.append(r)\n",
    "    sampleU, sampleI, sampleR = torch.LongTensor(sampleU), torch.LongTensor(sampleI), torch.Tensor(sampleR)\n",
    "    # loss calculation\n",
    "    loss = model.loss(sampleU, sampleI, sampleR)\n",
    "    # gradient calculation\n",
    "    loss.backward()\n",
    "    # weight update\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "for i in range(50):\n",
    "    loss = trainingStepBiasOnly(modelBiasOnly, interactionsTrain)\n",
    "    if (i % 10 == 9): print(\"iteration \" + str(i+1) + \", loss = \" + str(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the MSEs for a model which always predicts the mean, versus one which involves bias terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3254129874908986"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)\n",
    "\n",
    "alwaysPredictMean = [mu for _ in interactionsTest]\n",
    "labels = [r for _,_,r in interactionsTest]\n",
    "\n",
    "MSE(alwaysPredictMean, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "biasOnlyPredictions = \\\n",
    "    [modelBiasOnly(torch.LongTensor([userIDs[u]]), \\\n",
    "         torch.LongTensor([itemIDs[i]])).item() for u,i,_ in interactionsTest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.969268321990967"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biasOnlyPredictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9567523000636151"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(biasOnlyPredictions, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 (`TensorFlow` Implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance of a complete latent factor model (the latent factor model implementation is as the same as LatentFactorMOdel in the examples above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentFactorModel(tf.keras.Model):\n",
    "    def __init__(self, mu, K, lamb):\n",
    "        super(LatentFactorModel, self).__init__()\n",
    "        # Initialize to average\n",
    "        self.alpha = tf.Variable(mu)\n",
    "        # Initialize to small random values\n",
    "        self.betaU = tf.Variable(tf.random.normal([len(userIDs)],stddev=0.001))\n",
    "        self.betaI = tf.Variable(tf.random.normal([len(itemIDs)],stddev=0.001))\n",
    "        self.gammaU = tf.Variable(tf.random.normal([len(userIDs),K],stddev=0.001))\n",
    "        self.gammaI = tf.Variable(tf.random.normal([len(itemIDs),K],stddev=0.001))\n",
    "        self.lamb = lamb\n",
    "\n",
    "    # Prediction for a single instance (useful for evaluation)\n",
    "    def predict(self, u, i):\n",
    "        p = self.alpha + self.betaU[u] + self.betaI[i] +\\\n",
    "            tf.tensordot(self.gammaU[u], self.gammaI[i], 1)\n",
    "        return p\n",
    "\n",
    "    # Regularizer\n",
    "    def reg(self):\n",
    "        return self.lamb * (tf.reduce_sum(self.betaU**2) +\\\n",
    "                            tf.reduce_sum(self.betaI**2) +\\\n",
    "                            tf.reduce_sum(self.gammaU**2) +\\\n",
    "                            tf.reduce_sum(self.gammaI**2))\n",
    "    \n",
    "    # Prediction for a sample of instances\n",
    "    def predictSample(self, sampleU, sampleI):\n",
    "        u = tf.convert_to_tensor(sampleU, dtype=tf.int32)\n",
    "        i = tf.convert_to_tensor(sampleI, dtype=tf.int32)\n",
    "        beta_u = tf.nn.embedding_lookup(self.betaU, u)\n",
    "        beta_i = tf.nn.embedding_lookup(self.betaI, i)\n",
    "        gamma_u = tf.nn.embedding_lookup(self.gammaU, u)\n",
    "        gamma_i = tf.nn.embedding_lookup(self.gammaI, i)\n",
    "        pred = self.alpha + beta_u + beta_i +\\\n",
    "               tf.reduce_sum(tf.multiply(gamma_u, gamma_i), 1)\n",
    "        return pred\n",
    "    \n",
    "    # Loss\n",
    "    def call(self, sampleU, sampleI, sampleR):\n",
    "        pred = self.predictSample(sampleU, sampleI)\n",
    "        r = tf.convert_to_tensor(sampleR, dtype=tf.float32)\n",
    "        return tf.nn.l2_loss(pred - r) / len(sampleR)\n",
    "\n",
    "def trainingStep(model, interactions):\n",
    "    Nsamples = 50000\n",
    "    with tf.GradientTape() as tape:\n",
    "        sampleU, sampleI, sampleR = [], [], []\n",
    "        for _ in range(Nsamples):\n",
    "            u,i,r = random.choice(interactions)\n",
    "            sampleU.append(userIDs[u])\n",
    "            sampleI.append(itemIDs[i])\n",
    "            sampleR.append(r)\n",
    "\n",
    "        loss = model(sampleU,sampleI,sampleR)\n",
    "        loss += model.reg()\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients((grad, var) for\n",
    "                              (grad, var) in zip(gradients, model.trainable_variables)\n",
    "                              if grad is not None)\n",
    "    return loss.numpy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(0.1)\n",
    "modelLFM = LatentFactorModel(mu, 10, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10, objective = 0.539807\n",
      "iteration 20, objective = 0.5344611\n",
      "iteration 30, objective = 0.532373\n",
      "iteration 40, objective = 0.53467935\n",
      "iteration 50, objective = 0.5289248\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    obj = trainingStep(modelLFM, interactionsTrain)\n",
    "    if (i % 10 == 9): print(\"iteration \" + str(i+1) + \", objective = \" + str(obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [modelLFM.predict(userIDs[u],itemIDs[i]).numpy() for u,i,_ in interactionsTest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0097459352107794"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(predictions, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(probably needs a little more tuning in terms of number of latent factors, learning rate, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 (`PyTorch` Implementation)\n",
    "Performance of a complete latent factor model (the latent factor model implementation is as the same as LatentFactorMOdel in the examples above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10, loss = 1.0803641080856323\n",
      "iteration 20, loss = 1.0124554634094238\n",
      "iteration 30, loss = 0.9824608564376831\n",
      "iteration 40, loss = 0.9526613354682922\n",
      "iteration 50, loss = 0.9274086952209473\n"
     ]
    }
   ],
   "source": [
    "class LatentFactorModel(nn.Module):\n",
    "    def __init__(self, mu, K, lamb):\n",
    "        super(LatentFactorModel, self).__init__()\n",
    "        # Initialize to average\n",
    "        self.alpha = nn.Parameter(torch.Tensor([mu]))\n",
    "        # Initialize to small random values\n",
    "        self.betaU = nn.Embedding(len(userIDs), 1)\n",
    "        nn.init.normal_(self.betaU.weight, std=0.001)\n",
    "        self.betaI = nn.Embedding(len(itemIDs), 1)\n",
    "        nn.init.normal_(self.betaI.weight, std=0.001)\n",
    "        self.gammaU = nn.Embedding(len(userIDs), K)\n",
    "        nn.init.normal_(self.gammaU.weight, std=0.001)\n",
    "        self.gammaI = nn.Embedding(len(itemIDs), K)\n",
    "        nn.init.normal_(self.gammaI.weight, std=0.001)\n",
    "        self.lamb = lamb\n",
    "\n",
    "    # Prediction\n",
    "    def forward(self, u, i):\n",
    "        p = self.alpha + self.betaU(u).squeeze(-1) + self.betaI(i).squeeze(-1) + \\\n",
    "            (self.gammaU(u) * self.gammaI(i)).sum(dim=-1)\n",
    "        return p\n",
    "\n",
    "    # Regularizer\n",
    "    def reg(self):\n",
    "        return self.lamb * (torch.sum(self.betaU.weight**2) + \\\n",
    "                            torch.sum(self.betaI.weight**2) + \\\n",
    "                            torch.sum(self.gammaU.weight**2) + \\\n",
    "                            torch.sum(self.gammaI.weight**2))\n",
    "    \n",
    "    # Loss\n",
    "    def loss(self, u, i, r):\n",
    "        pred = self.forward(u, i)\n",
    "        return nn.functional.mse_loss(pred, r) + self.reg() # mse_loss in pytorch is 2 * l2_loss in tensorflow\n",
    "\n",
    "modelLFM = LatentFactorModel(mu, 10, 0.00001)\n",
    "optimizer = torch.optim.Adam(modelLFM.parameters(), lr=0.1)\n",
    "\n",
    "def trainingStep(model, interactions):  \n",
    "    # gradient reset\n",
    "    optimizer.zero_grad() \n",
    "    # data generation\n",
    "    Nsamples = 50000\n",
    "    sampleU, sampleI, sampleR = [], [], []\n",
    "    for _ in range(Nsamples):\n",
    "        u,i,r = random.choice(interactions)\n",
    "        sampleU.append(userIDs[u])\n",
    "        sampleI.append(itemIDs[i])\n",
    "        sampleR.append(r)\n",
    "    sampleU, sampleI, sampleR = torch.LongTensor(sampleU), torch.LongTensor(sampleI), torch.Tensor(sampleR)\n",
    "    # loss calculation\n",
    "    loss = model.loss(sampleU, sampleI, sampleR)\n",
    "    # gradient calculation\n",
    "    loss.backward()\n",
    "    # weight update\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "for i in range(50):\n",
    "    loss = trainingStep(modelLFM, interactionsTrain)\n",
    "    if (i % 10 == 9): print(\"iteration \" + str(i+1) + \", loss = \" + str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.020429780222474"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [modelLFM(torch.LongTensor([userIDs[u]]), torch.LongTensor([itemIDs[i]])).item() for u,i,_ in interactionsTest]\n",
    "MSE(predictions, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with rounding the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsRounded = [int(p + 0.5) for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0911789652247668"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(predictionsRounded, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to result in worse performance. For a rough explanation, consider a random variable that takes a value of \"1\" half the time and \"2\" half the time; in terms of the MSE, always predicting 1.5 (and always incurring moderate errors) is preferable to always predicting either of 1 or 2 (and incurring a large error half the time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 (`TensorFlow` Implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the BPR code from examples above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPRbatch(tf.keras.Model):\n",
    "    def __init__(self, K, lamb):\n",
    "        super(BPRbatch, self).__init__()\n",
    "        # Initialize variables\n",
    "        self.betaI = tf.Variable(tf.random.normal([len(itemIDs)],stddev=0.001))\n",
    "        self.gammaU = tf.Variable(tf.random.normal([len(userIDs),K],stddev=0.001))\n",
    "        self.gammaI = tf.Variable(tf.random.normal([len(itemIDs),K],stddev=0.001))\n",
    "        # Regularization coefficient\n",
    "        self.lamb = lamb\n",
    "\n",
    "    # Prediction for a single instance\n",
    "    def predict(self, u, i):\n",
    "        p = self.betaI[i] + tf.tensordot(self.gammaU[u], self.gammaI[i], 1)\n",
    "        return p\n",
    "\n",
    "    # Regularizer\n",
    "    def reg(self):\n",
    "        return self.lamb * (tf.nn.l2_loss(self.betaI) +\\\n",
    "                            tf.nn.l2_loss(self.gammaU) +\\\n",
    "                            tf.nn.l2_loss(self.gammaI))\n",
    "    \n",
    "    def score(self, sampleU, sampleI):\n",
    "        u = tf.convert_to_tensor(sampleU, dtype=tf.int32)\n",
    "        i = tf.convert_to_tensor(sampleI, dtype=tf.int32)\n",
    "        beta_i = tf.nn.embedding_lookup(self.betaI, i)\n",
    "        gamma_u = tf.nn.embedding_lookup(self.gammaU, u)\n",
    "        gamma_i = tf.nn.embedding_lookup(self.gammaI, i)\n",
    "        x_ui = beta_i + tf.reduce_sum(tf.multiply(gamma_u, gamma_i), 1)\n",
    "        return x_ui\n",
    "\n",
    "    def call(self, sampleU, sampleI, sampleJ):\n",
    "        x_ui = self.score(sampleU, sampleI)\n",
    "        x_uj = self.score(sampleU, sampleJ)\n",
    "        return -tf.reduce_mean(tf.math.log(tf.math.sigmoid(x_ui - x_uj)))\n",
    "\n",
    "def trainingStepBPR(model, interactions):\n",
    "    Nsamples = 50000\n",
    "    with tf.GradientTape() as tape:\n",
    "        sampleU, sampleI, sampleJ = [], [], []\n",
    "        for _ in range(Nsamples):\n",
    "            u,i,_ = random.choice(interactions) # positive sample\n",
    "            j = random.choice(items) # negative sample\n",
    "            while j in itemsPerUser[u]:\n",
    "                j = random.choice(items)\n",
    "            sampleU.append(userIDs[u])\n",
    "            sampleI.append(itemIDs[i])\n",
    "            sampleJ.append(itemIDs[j])\n",
    "\n",
    "        loss = model(sampleU,sampleI,sampleJ)\n",
    "        loss += model.reg()\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients((grad, var) for\n",
    "                              (grad, var) in zip(gradients, model.trainable_variables)\n",
    "                              if grad is not None)\n",
    "    return loss.numpy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(0.1)\n",
    "modelBPR = BPRbatch(10, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    obj = trainingStepBPR(modelBPR, interactionsTrain)\n",
    "    if (i % 10 == 9): print(\"iteration \" + str(i+1) + \", objective = \" + str(obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactionsTestPerUser = defaultdict(set)\n",
    "itemSet = set()\n",
    "for u,i,_ in interactionsTest:\n",
    "    interactionsTestPerUser[u].add(i)\n",
    "    itemSet.add(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AUCu(u, N): # N samples per user\n",
    "    win = 0\n",
    "    if N > len(interactionsTestPerUser[u]):\n",
    "        N = len(interactionsTestPerUser[u])\n",
    "    positive = random.sample(interactionsTestPerUser[u],N)\n",
    "    negative = random.sample(itemSet.difference(interactionsTestPerUser[u]),N)\n",
    "    for i,j in zip(positive,negative):\n",
    "        si = modelBPR.predict(userIDs[u], itemIDs[i]).numpy()\n",
    "        sj = modelBPR.predict(userIDs[u], itemIDs[j]).numpy()\n",
    "        if si > sj:\n",
    "            win += 1\n",
    "    return win/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AUC():\n",
    "    av = []\n",
    "    for u in interactionsTestPerUser:\n",
    "        av.append(AUCu(u, 10))\n",
    "    return sum(av) / len(av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7884310302103841"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 (`PyTorch` Implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10, loss = 0.5897132158279419\n",
      "iteration 20, loss = 0.5543973445892334\n",
      "iteration 30, loss = 0.5526553392410278\n",
      "iteration 40, loss = 0.547688364982605\n",
      "iteration 50, loss = 0.5417863130569458\n"
     ]
    }
   ],
   "source": [
    "class BPRbatch(nn.Module):\n",
    "    def __init__(self, K, lamb):\n",
    "        super(BPRbatch, self).__init__()\n",
    "        # Initialize to small random values\n",
    "        self.betaI = nn.Embedding(len(itemIDs), 1)\n",
    "        nn.init.normal_(self.betaI.weight, std=0.001)\n",
    "        self.gammaU = nn.Embedding(len(userIDs), K)\n",
    "        nn.init.normal_(self.gammaU.weight, std=0.001)\n",
    "        self.gammaI = nn.Embedding(len(itemIDs), K)\n",
    "        nn.init.normal_(self.gammaI.weight, std=0.001)\n",
    "        self.lamb = lamb\n",
    "\n",
    "    # Prediction\n",
    "    def forward(self, u, i):\n",
    "        p = self.betaI(i).squeeze(-1) + (self.gammaU(u) * self.gammaI(i)).sum(dim=-1)\n",
    "        return p\n",
    "\n",
    "    # Regularizer\n",
    "    def reg(self):\n",
    "        return self.lamb * (torch.sum(self.betaI.weight**2) + \\\n",
    "                            torch.sum(self.gammaU.weight**2) + \\\n",
    "                            torch.sum(self.gammaI.weight**2))\n",
    "    \n",
    "    # Loss\n",
    "    def loss(self, u, i, j):\n",
    "        x_ui = self.forward(u, i)\n",
    "        x_uj = self.forward(u, j)\n",
    "        return - torch.sigmoid(x_ui - x_uj).log().mean() + self.reg() # mse_loss in pytorch is 2 * l2_loss in tensorflow\n",
    "\n",
    "modelBPR = BPRbatch(10, 0.00001)\n",
    "optimizer = torch.optim.Adam(modelBPR.parameters(), lr=0.1)\n",
    "\n",
    "def trainingStepBPR(model, interactions):  \n",
    "    # gradient reset\n",
    "    optimizer.zero_grad() \n",
    "    # data generation\n",
    "    Nsamples = 50000\n",
    "    sampleU, sampleI, sampleJ = [], [], []\n",
    "    for _ in range(Nsamples):\n",
    "        u,i,_ = random.choice(interactions) # positive sample\n",
    "        j = random.choice(items) # negative sample\n",
    "        while j in itemsPerUser[u]:\n",
    "            j = random.choice(items)\n",
    "        sampleU.append(userIDs[u])\n",
    "        sampleI.append(itemIDs[i])\n",
    "        sampleJ.append(itemIDs[j])\n",
    "    sampleU, sampleI, sampleJ = torch.LongTensor(sampleU), torch.LongTensor(sampleI), torch.LongTensor(sampleJ)\n",
    "    # loss calculation\n",
    "    loss = model.loss(sampleU, sampleI, sampleJ)\n",
    "    # gradient calculation\n",
    "    loss.backward()\n",
    "    # weight update\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "for i in range(50):\n",
    "    loss = trainingStepBPR(modelBPR, interactionsTrain)\n",
    "    if (i % 10 == 9): print(\"iteration \" + str(i+1) + \", loss = \" + str(loss))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactionsTestPerUser = defaultdict(set)\n",
    "itemSet = set()\n",
    "for u,i,_ in interactionsTest:\n",
    "    interactionsTestPerUser[u].add(i)\n",
    "    itemSet.add(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AUCu(u, N): # N samples per user\n",
    "    win = 0\n",
    "    if N > len(interactionsTestPerUser[u]):\n",
    "        N = len(interactionsTestPerUser[u])\n",
    "    positive = random.sample(interactionsTestPerUser[u],N)\n",
    "    negative = random.sample(itemSet.difference(interactionsTestPerUser[u]),N)\n",
    "    for i,j in zip(positive,negative):\n",
    "        user, i, j = torch.LongTensor([userIDs[u]]), torch.LongTensor([itemIDs[i]]), torch.LongTensor([itemIDs[j]])\n",
    "        si = modelBPR(user, i).item()\n",
    "        sj = modelBPR(user, j).item()\n",
    "        if si > sj:\n",
    "            win += 1\n",
    "    return win/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AUC():\n",
    "    av = []\n",
    "    for u in interactionsTestPerUser:\n",
    "        av.append(AUCu(u, 10))\n",
    "    return sum(av) / len(av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7810019841269845"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
